{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import idx2numpy\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dims = 10\n",
    "image_dims = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = idx2numpy.convert_from_file('circles.idx')/127 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(np.prod(image_dims), activation='tanh'))\n",
    "model.add(Reshape(image_dims))\n",
    "\n",
    "noise = Input(shape=(noise_dims, ))\n",
    "image_out = model(noise)\n",
    "\n",
    "generator = Model(noise, image_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "image_in = Input(shape=image_dims)\n",
    "validity = model(image_in)\n",
    "discriminator = Model(image_in, validity)\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_in = Input(shape=(noise_dims,))\n",
    "image = generator(noise_in)\n",
    "\n",
    "discriminator.trainable = False\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "validity = discriminator(image)\n",
    "\n",
    "stacked = Model(noise_in, validity)\n",
    "\n",
    "stacked.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(samples):\n",
    "    n = len(samples)\n",
    "    fig, axes = plt.subplots(1, n)\n",
    "    \n",
    "    for ax, samples in zip(axes, samples):\n",
    "        ax.imshow(samples, cmap='Greys_r')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size=32):\n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dims))\n",
    "    real = np.ones((batch_size, 1))\n",
    "\n",
    "    return stacked.train_on_batch(noise, real)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(batch_size=32):\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    real = np.ones((batch_size, 1))\n",
    "    \n",
    "    \n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dims))\n",
    "    random_indices = np.random.choice(images.shape[0], batch_size)\n",
    "    \n",
    "    real_images = images[random_indices, :, :]\n",
    "    fake_images = generator.predict(noise)\n",
    "    \n",
    "    loss_fake, acc_fake = discriminator.train_on_batch(fake_images, fake)\n",
    "    loss_real, acc_real = discriminator.train_on_batch(real_images, real)\n",
    "    \n",
    "    return loss_fake, loss_real, acc_fake, acc_real\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch          0    loss stacked: 0.466    acc fake: 0.00000    acc real: 0.00000\n",
      "epoch        100    loss stacked: 6.911    acc fake: 0.69141    acc real: 0.08203\n",
      "epoch        200    loss stacked: 10.525    acc fake: 1.00000    acc real: 1.00000\n",
      "epoch        300    loss stacked: 4.032    acc fake: 1.00000    acc real: 0.73438\n",
      "epoch        400    loss stacked: 3.659    acc fake: 0.98047    acc real: 0.93359\n",
      "epoch        500    loss stacked: 4.465    acc fake: 1.00000    acc real: 0.96484\n",
      "epoch        600    loss stacked: 3.935    acc fake: 0.99609    acc real: 0.95703\n",
      "epoch        700    loss stacked: 2.035    acc fake: 0.80078    acc real: 0.84766\n",
      "epoch        800    loss stacked: 2.672    acc fake: 0.94922    acc real: 0.98828\n",
      "epoch        900    loss stacked: 4.425    acc fake: 0.99219    acc real: 0.99609\n",
      "epoch       1000    loss stacked: 13.849    acc fake: 1.00000    acc real: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "\n",
    "for epoch in range(8000):\n",
    "    stacked_loss = train_generator(256)\n",
    "    _, _, acc_fake, acc_real = train_discriminator(256)\n",
    "    \n",
    "    stats.append((stacked_loss, acc_fake, acc_real))\n",
    "    \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch:10}    loss stacked: {stacked_loss:2.3f}    acc fake: {acc_fake:0.5f}    acc real: {acc_real:0.5f}')\n",
    "\n",
    "        samples = generator.predict(np.random.normal(0, 1, (n_samples, noise_dims)))\n",
    "        history.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10)\n",
    "for row, samples in zip(axes, history[-10:]):\n",
    "    for ax, sample in zip(row, samples):\n",
    "        ax.imshow(sample, cmap='Greys_r')\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(20, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10)\n",
    "for ax, sample in zip(axes, images[:10]):\n",
    "    ax.imshow(sample, cmap='Greys_r')\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(20, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
