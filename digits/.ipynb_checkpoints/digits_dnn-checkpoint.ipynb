{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple DNN for the MNIST data set\n",
    "\n",
    "This code will train a 3-layer DNN on the MNIST data, and output a prediction for `input.bmp`. The image is in grayscale (white on black) and must have a size of 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = idx2numpy.convert_from_file('train-labels.idx')\n",
    "images = idx2numpy.convert_from_file('train-images.idx')\n",
    "\n",
    "images_flat =images.reshape((-1, 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = idx2numpy.convert_from_file('t10k-labels.idx')\n",
    "test_images = idx2numpy.convert_from_file('t10k-images.idx')\n",
    "\n",
    "test_images = test_images.reshape((-1, 28*28))  # flatten images for input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=784, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.1047 - acc: 0.3039 - val_loss: 1.4811 - val_acc: 0.4590\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 1.5735 - acc: 0.4033 - val_loss: 1.3687 - val_acc: 0.4820\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.5198 - acc: 0.4244 - val_loss: 1.2832 - val_acc: 0.5070\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.4711 - acc: 0.4431 - val_loss: 1.2854 - val_acc: 0.5410\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 1.4299 - acc: 0.4623 - val_loss: 1.2767 - val_acc: 0.5690\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.4064 - acc: 0.4725 - val_loss: 1.2088 - val_acc: 0.5600\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.3713 - acc: 0.4845 - val_loss: 1.1898 - val_acc: 0.5550\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.3626 - acc: 0.4839 - val_loss: 1.1481 - val_acc: 0.5750\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.3423 - acc: 0.4978 - val_loss: 1.0960 - val_acc: 0.5990\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.3306 - acc: 0.5090 - val_loss: 1.0951 - val_acc: 0.6280\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.2967 - acc: 0.5246 - val_loss: 1.1311 - val_acc: 0.6240\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.2573 - acc: 0.5393 - val_loss: 1.0708 - val_acc: 0.6120\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 1.2498 - acc: 0.5398 - val_loss: 1.0839 - val_acc: 0.6220\n",
      "Epoch 14/20\n",
      "32800/60000 [===============>..............] - ETA: 2s - loss: 1.2417 - acc: 0.5399"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c82be866cba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    images_flat,\n",
    "    keras.utils.to_categorical(labels),\n",
    "    validation_data=(test_images[:1000], keras.utils.to_categorical(test_labels[:1000])),\n",
    "    batch_size=32,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 33us/step\n",
      "Accuracy on rest of test data: 0.87\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(test_images[1000:], keras.utils.to_categorical(test_labels[1000:]))\n",
    "print(f'Accuracy on rest of test data: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# load input image into array\n",
    "input_image = Image.open('input.bmp')\n",
    "array = np.array(input_image.getdata())[:,0]\n",
    "\n",
    "x = np.arange(10) # digits\n",
    "y = model.predict(array.reshape((1, 784))).reshape(10) # categorical output\n",
    "\n",
    "# Draw input image\n",
    "plt.subplot(121)\n",
    "plt.imshow(array.reshape((28, 28)), cmap='Greys_r')\n",
    "\n",
    "# Show output\n",
    "plt.subplot(122)\n",
    "plt.xticks(x)\n",
    "plt.bar(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
